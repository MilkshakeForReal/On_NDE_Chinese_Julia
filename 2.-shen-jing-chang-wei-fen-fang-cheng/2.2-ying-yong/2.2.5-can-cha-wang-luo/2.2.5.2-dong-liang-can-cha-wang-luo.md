# 2.2.5.2 动量残差网络

\[San+21]考虑将通过残差网络的前向传播替换为

$$
\begin{aligned}
&v_{j+1}=\gamma v_{j}+(1-\gamma) f_{\theta}\left(y_{j}\right) \\
&y_{j+1}=y_{j}+v_{j+1}
\end{aligned}\tag{2.12}
$$

对于$$\ \gamma \in(0,1) .(\gamma=0.9$$比较经典)。

**可逆性** 此类网络的关键属性是它们是可逆的。虽然(2.12)从$$(v_j , y_j )$$计算出$$(v_{j+1}, y_{j+1})$$，但也有可从$$(v_{j+1}, y_{j+1})$$重建$$(v_{j}, y_{j})$$：

$$
\begin{aligned}
&y_{j}=y_{j+1}-v_{j+1} \\
&v_{j}=\frac{1}{\gamma}\left(v_{j+1}-(1-\gamma) f_{\theta}\left(y_{j}\right)\right) .
\end{aligned}\tag{2.13}
$$

这极大地提高了网络的内存效率，但也付出了一些额外的计算。当通过（2.12）进行反向传播时，中间值 $$y_n$$不需要被存储（就像它们对相应的残差网络那样）。 相反，（2.13）意味着它们可以在反向传播过程中按需重新计算。

这代表了\[Gom+17; Cha+18]中类似想法的细化，以及更广泛的可逆神经网络主题\[Beh+19]。

**作为一个神经ODE**  令$$\varepsilon=1 /(1-\gamma)$$，那么(2.12)是由半隐式欧拉方法给出的，具有单位步长，应用于

$$
\varepsilon \frac{\mathrm{d}^{2} y}{\mathrm{~d} t^{2}}(t)+\frac{\mathrm{d} y}{\mathrm{~d} t}(t)=f_{\theta}(y(t))\tag{2.14}
$$

> 译者注：令$$v_n=y'_n$$.

**与可逆求解器的联系**  动量网络是可逆的，因为半隐式欧拉方法是可逆的。在时间上向前运行求解器，然后再向后运行，将恢复相同的数值解。这有时被描述为说在前向和后向求解上存在匹配的截断误差。

强烈建议将可逆求解器用于神经微分方程，原因与此相同：它们允许反向传播，既节省时间又节省内存（第[5.3.2](../../../5.-shen-jing-wei-fen-fang-cheng-shu-zhi-jie/5.3-shu-zhi-qiu-jie-qi/5.3.2-ke-ni-qiu-jie-qi.md)节）。因此，它们具有很大的意义，而且一般来说不需要（2.14）所表现出的二阶结构。



