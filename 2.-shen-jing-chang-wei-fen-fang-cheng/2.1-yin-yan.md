# 2.1 引言

到目前为止，最常见的神经微分方程是一个神经ODE



$$
y(0)=y_{0} \quad \frac{\mathrm{d} y}{\mathrm{~d} t}(t)=f_{\theta}(t, y(t))\tag{2.1}
$$

其中$$y_{0} \in \mathbb{R}^{d_{1} \times \ldots \times d_{k}}$$是一个任意维度的张量，$$\theta$$代表一些学习到的参数向量，$$f_{\theta}: \mathbb{R} \times \mathbb{R}^{d_{1} \times \ldots \times d_{k}} \rightarrow \mathbb{R}^{d_{1} \times \ldots \times d_{k}}$$是一个神经网络。通常$$f_{\theta}$$会是一些标准的简单神经结构，如前馈或卷积网络。

## 2.1.1 存在性与唯一性

通常提出的第一个问题（至少是数学家）是关于方程（2.1）的解的存在性和唯一性。这是很直接的问题。只要$$f_{\theta}$$是Lipschitz连续——神经网络通常是的，它通常是Lipschitz函数的组合——那么Picard存在性定理就适用。

## 2.1.2 求值与训练

与非微分方程的模型相比，一般来说，有两个额外的问题必须牢记在心。

首先，我们必须能够获得微分方程的数值解。(基本上不会有解析解）。

其次，我们必须能够穿过微分方程反向传播，以获得其参数$$\theta$$的梯度。

执行这些任务的软件现在已经标准化了（第[5.6](../5.-shen-jing-wei-fen-fang-cheng-shu-zhi-jie/5.6-ruan-jian.md)节），所以我们可以自由地专注于构建模型结构本身的任务。[第5章](../5.-shen-jing-wei-fen-fang-cheng-shu-zhi-jie/)将对求值和反向传播进行更深入的研究。
