# 2.3.1 神经架构

几乎每项工作都对向量场$$f_{\theta}$$使用前馈或卷积神经网络。前馈网络很直接：简单地将$$t$$和$$y(t)$$连接起来作为输入。当数据不是图像时，通常会使用这些网络。

如果数据$$y_{0}$$具有图像的（通道、高度、宽度）结构，那么可以通过卷积层获得一个合适的向量场。回顾一下，$$f_{\theta}(t, \cdot)$$的输入和输出必须是相同大小。这通常意味着要么使用填充，要么将卷积层与转置卷积层相结合。时间$$t$$通常被附加到$$y(t)$$上作为一个额外的通道。

**评论2.8**  偶尔也会使用其他参数化。例如\[Pol+19; Cra+20a; Den+19; Cha+21]考虑了图神经网络，，它可以编码与输入点的排列有关的等值性。(例在许多物理系统中可能会表现出这样的情况；例如，在重力作用下演变的n个同等大小的质量块的位置）。

尽管我们在此不详细讨论图结构网络和图结构数据，但下面的许多讨论都贯穿于这一背景中。

### 2.3.1.1 激活函数

穿过ODE进行反向传播的理论在技术上确实要求向量场（以及激活函数）是连续可微的（第[5.1](../../5.-shen-jing-wei-fen-fang-cheng-shu-zhi-jie/5.1-chuan-guo-odes-jin-hang-fan-xiang-chuan-bo/)节），而ReLU不满足此条件。因此，连续可微的激活函数，如SiLU、softplus或tanh等连续可微的激活函数通常被使用。尽管有这一理论观点，ReLU的激活仍然经常在实践中成功使用。

> 有个很有趣的"squareplus"激活函数: $$x \mapsto \frac{1}{2} x+\frac{1}{2} \sqrt{x^{2}+4}$$

### 2.3.1.2 归一化

至少在向量场$$f_{\theta}$$内部，通常不使用归一化，如批量归一化和层归一化\[IS15; BKH16]。对于批量归一化，这是因为同一个神经网络$$f_{\theta}$$在不同的$$t$$下在$$y(t)$$求值，而对于每个$$t$$可能有不同的统计属性。这与在递归神经网络中使用批量归一化时出现的问题相同\[BKH16; Coo+17]。

同时层归一化对其缺乏效力缺乏令人满意的解释，但至少对于连续归一化流来说，有报道称这通常会破坏训练\[Che20]。

### 2.3.1.3 初始化

将神经向量场$$f_\theta$$初始化到接近零，往往能改善训练效果，与随机初始化动力系统相比，扰动一个接近常数的$$t \to y(t)$$更容易。对于大多数神经结构来说，这可以通过选择接近零的初始参数$$θ$$来实现。
