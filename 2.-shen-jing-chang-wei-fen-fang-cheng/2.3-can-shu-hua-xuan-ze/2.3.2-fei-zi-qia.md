# 2.3.2 非自洽

我们特意选择将$$t$$作为向量场$$f_{\theta}$$的输入。残差网络在不同深度有不同的层。类似地，神经ODE模型通常通过允许$$f_{\theta}$$依赖于 "连续深度 "参数$$t$$而表现出更高的建模能力。这样的微分方程被称为非自治的。

这可以通过将$$t$$和$$y(t)$$拼接起来作为$$f_{\theta}$$的输入来简单处理。一个更具表现力的选择是对某些时间依赖性进行明确的编码。

### 2.3.2.1 深度离散化：叠加

一种直接而有效的方法是将$$f_{\theta}$$分段参数化为几个不同的网络，根据$$t$$的值来选择。比如说，



$$
f_{\theta}(t, y)=\left\{\begin{array}{cl}
f_{\theta_{1}, 1}(t, y) & t \in\left[t_{0}, t_{1}\right] \\
\vdots & \\
f_{\theta_{n}, n}(t, y) & t \in\left[t_{n-1}, t_{n}\right]
\end{array}\right.
$$

其中$$\theta=\left(\theta_{1}, \ldots, \theta_{n}\right)$$，每个$$\theta_j$$自身都是某个参数向量。

原则上每个$$f_{\cdot, 1}, \ldots, f_{\cdot, n}$$可以代表不同的架构。通常$$f_{\cdot, j}$$都是相同的神经结构，区别只在于它们依赖的参数向量$$\theta_{i}$$。

在实践中使用这种架构时，使用数值微分方程求解器必须考虑两个选项：

1. 对$$\left[t_{0}, t_{n}\right]$$进行一次调用ODE求解器；
2. 对每个$$\left[t_{i}, t_{i+1}\right]$$区间分别求解，并调用ODE求解器$$n$$次。

这两种选择都是有效的，但都引入了人们应该注意的细节；我们将这种数值讨论推迟到第5.3.3节。

### 2.3.2.2 谱离散化

令$$\psi_{j}:[0, T] \rightarrow \mathbb{R}$$为一（光滑）函数族，由$$j \in\{1, \ldots, n\}$$参数化。以参数向量$$\theta$$为例，$$\theta=\left({1}, \ldots, \theta_{n}\right)$$，而$$\theta_{j} \in \mathbb{R}^{d_{\theta}}$$ 对于某个 $$d_{\theta}\in \mathbb{N}$$ 。现在定义



$$
\alpha_{\theta}(t)=\sum_{j=1}^{n} \theta_{j} \psi_{j}(t).
$$

> 译者注：实际上是对$$\{(1,\theta_1),(2,\theta_2),\dots,(n,\theta_n)\}$$进行插值。

那么，另一种非自洽的选择是由以下方式给出的

$$
f_{\theta}(t, y(t))=\widetilde{f}_{\alpha_{\theta}(t)}(t, y(t))
$$

其中$$\widetilde{f}$$是某固定的神经网络架构，在时间$$t$$使用参数$$α_θ(t)∈R$$。

$$ψ_j$$的选择由我们决定。理想的情况是，它们应该相互之间有很大的不同，以便于模型的最大可能的表达能力。例如，它们可以被选为切比雪夫多项式，或者是正弦和余弦的截断傅里叶基（这就是术语 "谱离散化 "的动机\[Mas+20])。

### 2.3.2.3 超网络

另一个选择是让神经ODE的参数本身作为神经ODE的参数化的解。

也就是说，让$$\alpha:[0, T] \rightarrow \mathbb{R}^{d_{\alpha}}$$为神经ODE



$$
\alpha(0)=\alpha_{\theta} \quad \frac{\mathrm{d} \alpha}{\mathrm{d} t}(t)=g_{\theta}(t, \alpha(t))
$$

的解，参数$$θ$$，矢量场$$g_θ : R × R^{dα} → R^{dα}$$，以及学习到的初始条件$$α_θ$$。然后，我们让我们的 "原始 "神经ODE的隐藏态$$y$$按照以下方式演化

$$
y(0)=y_{0} \quad \frac{\mathrm{d} y}{\mathrm{~d} t}(t)=\widetilde{f}_{\alpha(t)}(t, y(t))
$$

其中$$\widetilde{f}$$是某固定神经网络架构，在时间$$t$$使用参数$$α(t)∈R^{dα}$$。

在实践中，这两个微分方程可以被拼接起来，作为一整个系统同时求解。总的来说，这可以被视为只是一个最初的神经ODE，其向量场有一个特殊的有益结构。见e \[Zha+19; Cho+20]。

### 2.3.2.4 变体层

其他高性能的依赖时间的层可以被想象出来。例如，在第[2.2.3.3](../2.2-ying-yong/2.2.3-lian-xu-gui-yi-hua-liu.md)节中看到的CNF例子使用了一个MLP，其 其仿s射层被替换成以下形式的层



$$
(x, t) \mapsto(A x+b) * \sigma(c t+d)+e t
$$

其中$$x \in \mathbb{R}^{d_{1}}, t \in \mathbb{R}, A \in \mathbb{R}^{d_{2} \times d_{1}}, b, c, d, e \in \mathbb{R}^{d_{2}}$$，$$σ$$表示sigmoid函数，∗表示元素乘法。

对时间$$t$$的依赖是在MLP的每一层进来的，而不是与$$y(t)$$拼接起来作为另一个输入。

这让人想起GRU和LSTM的门控程序。

### 2.3.2.5 强制自洽

在使用神经ODEs方程处理时间序列问题时，有时会出现上述程序的一个例外，比如隐ODE（[2.2.4](../2.2-ying-yong/2.2.4-yin-odes.md)节）。在这种情况下，我们有时可能会假设背后的动力学不具有时间依赖性，而倾向于删除$$t$$作为输入。(在[第三章](../../3.-shen-jing-kong-zhi-wei-fen-fang-cheng.md)和[第四章](../../4.-shen-jing-sui-ji-wei-fen-fang-cheng.md)中，即将出现的神经CDEs和神经SDEs也往往是这样的情况）。



