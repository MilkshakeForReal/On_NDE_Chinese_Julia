# 5.1.1 先离散再优化

第一个选项是简单地通过微分方程求解器的内部算符进行反向传播。

微分方程求解器在内部执行加法、乘法等常规算术运算，每一种运算都是可微的。鉴于求解操作是可微分操作的组合，它也是可微分的。 这就是所谓的 "离散-优化"。导数的计算是相对于求解器对应计算的离散化微分方程而言的，而不是相对于理想化的连续时间方程而言的。

### 5.1.1.1 优点

**梯度的准确性**  对于实际使用的离散模型，计算的梯度将是准确的。这与我们将在后面看到的一些技术相反，这些技术只计算近似的梯度。

**速度**  这通常是最快的反向传播方式。其中一个原因是，在执行反向传播之前，完整的计算图是已知的，因此底层的自动分化库可以更好地利用并行性。

**实现的简易性**  离散-优化的实施通常很简单：只要微分方程求解器是用自动微分框架（如PyTorch或JAX）编写的，那么梯度可以按照这些框架的通常方式自动计算。

### 5.1.1.2 缺点

**内存效率低**  这种方法内存效率低，因为解算器的每一个内部操作都必须被记录下来。如果记录一个微分方程步骤的操作的内存成本是H，并且回顾一下T是时间范围，那么这种方法就会消耗O(HT)内存。

这与我们将在后面看到的技术形成对比，后者将其减少到只有 O(H)。 备注5.1. 从某种意义上说，说 "离散-优化 "的内存效率很低是有点不公平的。它只是像其他神经网络模型一样正常地执行反向传播，而我们通常不会把这些模型称为内存效率低下。只是我们后面看到的其他选项可以将内存成本降低到基本可以忽略不计的程度。 实施的难度 与刚才讨论的 "实施的难度 "相反的是 如果提供的微分方程求解器没有在自微分框架中编写，那么这种方法基本上是不可能实现的。

### 5.1.1.3 检查点

有可能通过检查点来解决内存效率低下的问题。也就是说，在求解过程中的某些点上记录前向传递的值，并在后向传递过程中使用这些值来重构。这是深度学习中的一项通用技术\[Gri92]。\[GKB19]在神经ODEs的特定背景下讨论了这一点。
