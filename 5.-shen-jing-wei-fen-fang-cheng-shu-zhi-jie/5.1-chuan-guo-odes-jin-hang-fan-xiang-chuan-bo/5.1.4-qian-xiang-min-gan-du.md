# 5.1.4 前向敏感度

虽然在技术上不是**反向**传播，但为了完整起见，我们提到它也可以计算ODE的前向敏感性。(或是CDE或SDE)。

再一次，这可以以先离散后优化的方式或先优化后离散的方式进行。

离散-优化是通过计算求解器计算图的前向敏感性来完成的。

优化-离散是通过简单地对方程（5.3）中的$$y_0$$和$$θ$$进行微分实现的 。通过这个方法我们可以得到以下定理：

> 译者注1：这两者又被分别称为离散前向敏感度和连续前向敏感度。离散前向敏感度也可以通过前向自动微分实现。

**定理5.8**  令$$y_{0} \in \mathbb{R}^{d}, \theta \in \mathbb{R}^{m}, f_{\theta}:[0, T] \times \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$$关于时间$$t$$连续，关于$$y$$一致李普西茨连续且连续可微。令$$y:[0, T] \rightarrow \mathbb{R}^{d}$$为

$$
y(0)=y_{0}, \quad \frac{\mathrm{d} y}{\mathrm{~d} t}(t)=f_{\theta}(t, y(t))\tag{5.3}
$$

的唯一解。那么$$\frac{\mathrm{d} y(t)}{\mathrm{d} y_{0}}=J_{y}(t)$$和$$\frac{\mathrm{d} y(t)}{\mathrm{d} \theta}=J_{\theta}(t)$$，其中$$J_{y}:[0, T] \rightarrow \mathbb{R}^{d \times d}$$和$$J_{\theta}:[0, T] \rightarrow \mathbb{R}^{d \times m}$$满足如下微分方程

$$
\begin{aligned} J_{y}(0) &=I_{d \times d}, & \frac{\mathrm{d} J_{y}}{\mathrm{~d} t}(t) &=\frac{\partial f_{\theta}}{\partial y}(t, y(t)) J_{y}(t) \\ J_{\theta}(0) &=0, & \frac{\mathrm{d} J_{\theta}}{\mathrm{d} t}(t) &=\frac{\partial f_{\theta}}{\partial y}(t, y(t)) J_{\theta}(t)+\frac{\partial f_{\theta}}{\partial \theta}(t, y(t)) \end{aligned}
$$

右手边由雅可比-向量积组成，可以通过自动微分高效地计算出来。

像往常一样，在考虑具有许多参数的机器学习问题时，前向灵敏度通常不如反向模式的自动微分效率高，所以这种方法不常被使用。\[Rac+20a]报告说，它（仅）在参数很少（<100）的问题上是有用的。

> 译者注2：我们可以看出来，对每个标量参数都要求解一个标量微分方程，这也是敏感度分析不适用于大量参数的原因。

> 译者注3：让我们总结一下第5.1.2节。求解损失函数对参数的梯度有如下方法
>
> 1. 前向敏感度：适用于参数量小的情况
>    1. 连续敏感度：求解敏感度微分方程。
>    2. 离散敏感度：对数值求解器使用前向自动微分。
> 2. （后向）伴随法：适用于参数量大的情况
>    1. 连续伴随法：求解伴随微分方程。
>    2. 离散伴随法：对数值求解器使用反向自动微分。
