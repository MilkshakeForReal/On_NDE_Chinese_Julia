# 5.3.1 开箱即用的数值求解器

神经网络表示非结构化的矢量场。这意味着许多更专业的微分方程求解器（为任何特定方程开发）并不适用。 并不适用，我们必须依靠 "通用 "求解器。 这类数值微分方程求解器的文献非常丰富。我们将主要关注显式Runge-Kutta求解器，特别是针对ODEs和CDEs的求解器，它是一种流行的数值求解器系列。还有其他合理的选择--例如线性多步方法--但我们的目的不是重述数值微分方程文献。

### 5.3.1.1 一般原则&#x20;

有一些原则是专门针对神经微分方程而不是一般的微分方程的，它们有助于指导数值求解器的选择。

**隐式求解器**  隐式求解器，例如，隐式欧拉方法$$y_{j+1} = yj + ∆tf(t_{j+1}, y_{j+1})$$，很少使用。 它们的计算成本很高：隐式求解器在每一步都要解决一个线性或非线性系统，通常是通过不动点迭代。神经微分方程是一个矢量场求值很昂贵的系统，而且许多矢量场求值已经在进行了（在一个批次和训练过程中）。 降低计算成本是很有意义的。&#x20;

隐式求解器的一个主要用例是解决刚性微分方程。然而，刚性往往不是神经微分方程的问题——如果刚性和显式求解器产生一个糟糕的解，那么模型和数据之间的损失可能很大。由于这是我们明确训练以避免的标准（实现小的损失），那么这个问题就避免了。

> **备注5.13** 上述描述对于 "机器学习神经微分方程"（如CNFs或神经CDEs--分别为第[2.2.3](../../2.-shen-jing-chang-wei-fen-fang-cheng/2.2-ying-yong/page-3.md)节和第[3](../../3.-shen-jing-kong-zhi-wei-fen-fang-cheng.md)章）来说是典型的，但它并不是一个普遍的规则。例如，如果矢量场包含了已知的结构（第[2.2.2](../../2.-shen-jing-chang-wei-fen-fang-cheng/2.2-ying-yong/2.2.2-wu-li-jian-mo-he-gui-na-pian-zhi.md)节），或者数据有多个不同的时间尺度，那么刚性可能是不可避免的，隐式求解器可能成为一个合理的选择。例如，见\[Kim+21a]。

> 脚注：有点一词多义，因为刚性微分方程被广泛归类为 "显式求解器失效的方程"。

**自适应与固定步长求解器**  固定步长和自适应步长的求解器通常都是神经微分方程的合理选择。

给定一个时间范围$$T>0$$，那么固定步长求解器提前选择一些步长位置$$0 = t_0 < t_1 < . ..< t_n = T$$，通常$$Δt = t_{j+1 }- t_j$$与$$j$$无关。

自适应步骤求解器改变下一步$$t_{j+1}-t_j$$的大小。 以使求解过程中的（局部）误差近似等于某个容忍度。例如，嵌入式Runge-Kutta方法就属于这种类型。这意味着可变的计算成本，通常在训练过程中随着模型复杂性的增加而增加\[Che+18b, 图3(d)], \[Fin+20a, 图3(c)]。

> 宽泛地说，神经网络在训练过程中往往会增加复杂性\[Kal+19]，\[JGH18，第5节]。这表现为在训练过程中，训练和验证损失遵循经典的偏差-变异曲线。

**例5.14**  考虑用密集采样和缓慢变化的时间序列数据作为输入来解一个神经CDE。输入数据的缓慢变化意味着处理它的每一个片段——就像我们用RNN做的那样——可能是多余的。求解器的自适应性可以自动检测微分方程驱动的缓慢时间尺度，并产生适当大小的积分步长，大于数据的离散化。&#x20;

此外，RNN训练经常随着时间序列长度的增加而中断。 如果这个长度是通过对同一信号进行越来越密集的采样来实现的，那么这个额外的信息会导致我们的模型训练失败，这就有点不正常了。从哲学上讲，能够使用自适应求解器来克服这个问题是令人欣慰的。

**嵌入离散化**  如果在模型训练中只使用一个求解器（特别是低阶求解器或固定步长的求解器），那么这种离散化的选择可能成为模型的内在组成部分。神经矢量场将被训练成在这种离散化下工作得最好，而在其他离散化下可能会失败\[Ott+21; Que+21]。

对于许多应用来说，这并不是一个问题。在本论文的介绍中（第[1.2](../../1.-yin-yan/1.2-shen-jing-wei-fen-fang-cheng-an-li/)节），"离散化模型的灵感 "被描述为神经微分方程的一个很好的使用案例，嵌入数值离散化中只是其中一个微妙的例子。

**步长和误差容限**   步长（对于固定步长求解器）或误差容限（对于自适应步长求解器）与通常的数值微分方程文献中所见到的相比，往往会非常大。例如，当使用神经SDE生成一个在某些点$$t_{0}<\cdots<t_{n}$$采样的时间序列时，那么我们可以选择在每个区间$$[t_j , t_{j+1}]$$采取单一的数值步骤，即使$$t_{j+1} - t_j$$比较大。 这也可以被认为是把连续模型作为一个理想，然后故意拟合一个离散模型。较大的步长往往是出于降低计算成本的考虑，从而减少训练时间。

**例5.15**  在这个 "大步长格式 "中，请注意，采取较小的步长可能会产生稍微更强表达力的模型。当希望增加或减少神经微分方程的建模能力时，步长和向量场的复杂性都是可以调整的选项。 作为一个例子，考虑像第[四](../../4.-shen-jing-sui-ji-wei-fen-fang-cheng.md)章中那样拟合一个神经SDE。如果使用Euler-Maruyama方法采取单一的数值步骤，那么$$y(t_{j+1})|y(t_j)$$的条件分布将只是高斯的，这是相对不具表达力的。

**求解器的阶数**  低阶求解器通常是合理的选择，特别是在大步长格式中明确嵌入离散化的时候。例如，欧拉方法是一阶收敛的；中点或Heun方法是二阶收敛的。 如果旨在拟合理想化的连续模型（例如通过先优化后离散的训练），那么高阶求解器如Dormand-Prince通常是首选。(至少当它们可用时，这是对于ODEs和CDEs，而不是SDEs。）
