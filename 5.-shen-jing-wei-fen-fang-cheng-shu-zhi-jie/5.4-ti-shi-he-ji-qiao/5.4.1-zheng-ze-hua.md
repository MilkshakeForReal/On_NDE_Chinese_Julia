# 5.4.1 正则化

### 5.4.1.1 权重衰减

给神经向量场的参数添加权重衰减可能有助于提高模型性能，就像在传统的深度学习中一样。 对于许多神经网络来说，其输出的范围与权重的范围大致成正比。也就是说，$$\left\|f_{\theta}\right\| /\|\theta\|$$在不同的$$θ$$值上可能是近似常数。因此，权重衰减的另一个含义是，向量场可能更接近于零，因此在数值上更容易积分。

### 5.4.1.2 时间正则化

对于神经微分方程在 "非时间序列 "问题上的应用，正则化的一种形式是随机地选择积分的区域。例如，当训练一个连续的归一化流时，不采取固定的积分区域$$[τ, T]$$，而是从一些分布中抽出端点$$τ , T$$。

也许$$τ=0$$保持固定，而$$T∼Uniform[0.9, 1.1]$$。这在计算上是很便宜的，同时鼓励模型对小的扰动具有鲁棒性\[Gho+20]。

### 5.4.1.3 加性噪声&#x20;

主流的深度学习经常使用随机性作为正则器，例如dropout。 相应地，具体到神经ODEs和神经CDEs，那么在每一步之后包括一些小的加性噪声（使模型成为一个SDE）是另一个计算便宜的选择，鼓励更多的鲁棒模型\[OVV20; Cra21b]。 在这种情况下，添加的噪声应该是固定的。如果它是学习的，那么训练过程将把它缩小到零，并且不应用正则化。
