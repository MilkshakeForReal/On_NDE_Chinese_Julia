# 1.1.4 连续深度的神经网络

我们刚刚看到神经微分方程如何通过传统的数学建模来处理。它们也可以通过现代深度学习来实现。

回顾一下残差网络的公式

$$
y_{j+1}=y_{j}+f_{\theta}\left(j, y_{j}\right)\tag{1.1}
$$

其中$$f_{\theta}(j, \cdot)$$为第$$j$$个残差块。（每个残差块的参数拼在一个构成了$$\theta$$。）

现在回忆一下神经ODE



$$
\frac{\mathrm{d} y}{\mathrm{~d} t}(t)=f_{\theta}(t, y(t))
$$

通过显式欧拉方法在均匀相隔$$∆t$$的时间$$t_j$$上对其进行离散化，得到



$$
\frac{y\left(t_{j+1}\right)-y\left(t_{j}\right)}{\Delta t} \approx \frac{\mathrm{d} y}{\mathrm{~d} t}\left(t_{j}\right)=f_{\theta}\left(t_{j}, y\left(t_{j}\right)\right)
$$

所以



$$
y\left(t_{j+1}\right)=y\left(t_{j}\right)+\Delta t f_{\theta}\left(t_{j}, y\left(t_{j}\right)\right)
$$

将$$∆t$$吸收到$$f_θ$$中，我们恢复了方程（1.1）的表述。

在做出此观察后——神经ODEs是残差网络的连续极限——我们可能会被促使开始建立其他联系。

结果发现，GRU或LSTM的关键特征是更新规则，它们看起来很像离散微分方程（[第三章](../../3.-shen-jing-kong-zhi-wei-fen-fang-cheng.md)）。StyleGAN2和（基于分数的）扩散模型只是离散化的SDE（[第4章](../../4.-shen-jing-sui-ji-wei-fen-fang-cheng.md)）。可逆神经网络中的耦合层原来与可逆微分方程求解器有关（[第五章](../../5.-shen-jing-wei-fen-fang-cheng-shu-zhi-jie/)），等等。

巧合的是（或者说，随着这个想法变得越来越流行，是设计出来的），许多最有效和最流行的深度学习架构类似于微分方程。也许我们不应该感到惊讶：几个世纪以来，微分方程一直是主流的建模范式；它们不那么容易被推翻。
