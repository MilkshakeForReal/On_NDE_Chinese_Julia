# 3. MNIST图像分类

### 导入相关库

```julia
using Lux
using ComponentArrays, CUDA, DiffEqSensitivity, NNlib, Optimisers, OrdinaryDiffEq, Random,
      Statistics, Zygote
import MLDatasets: MNIST
import MLDataUtils: convertlabel, LabelEnc
import MLUtils: DataLoader, splitobs
CUDA.allowscalar(false)
```

这里，`Lux` 是我们要使用的深度学习框架，它的好处在于每个神经网络层都是纯函数，参数是显式传入的，我们会看到这样的代码

```
y, st = model(x,ps,st)
```

这里`x` 是输入，`ps` 是参数，`st` 是状态，它用来记录一些信息，例如方差等。这样的好处是将多个神经层的参数打包成一个向量很容易，方便用于Neural ODE计算梯度。

### 加载数据集

```
# Use MLDataUtils LabelEnc for natural onehot conversion
function onehot(labels_raw)
    return convertlabel(LabelEnc.OneOfK, labels_raw, LabelEnc.NativeLabels(collect(0:9)))
end

function loadmnist(batchsize, train_split)
    # Load MNIST: Only 1500 for demonstration purposes
    N = 1500
    imgs = MNIST.traintensor(1:N)
    labels_raw = MNIST.trainlabels(1:N)

    # Process images into (H,W,C,BS) batches
    x_data = Float32.(reshape(imgs, size(imgs, 1), size(imgs, 2), 1, size(imgs, 3)))
    y_data = onehot(labels_raw)
    (x_train, y_train), (x_test, y_test) = splitobs((x_data, y_data); at=train_split)

    return (
            # Use DataLoader to automatically minibatch and shuffle the data
            DataLoader(collect.((x_train, y_train)); batchsize=batchsize, shuffle=true),
            # Don't shuffle the test data
            DataLoader(collect.((x_test, y_test)); batchsize=batchsize, shuffle=false))
end
```

这段代码不用看。

#### 定义NeuralODE

```julia
struct NeuralODE{M <: Lux.AbstractExplicitLayer, So, Se, T, K} <:
       Lux.AbstractExplicitContainerLayer{(:model,)}
    model::M
    solver::So
    sensealg::Se
    tspan::T
    kwargs::K
end
```

解释：

* &#x20;`model` : 表示微分方程右端的神经网络
* `solver` ：表示ODE求解器
* `sensealg` ：表示计算灵敏度的办法。也就是我们在[5.1](../5.-shen-jing-wei-fen-fang-cheng-shu-zhi-jie/5.1-chuan-guo-odes-de-fan-xiang-chuan-bo.md)节提到的各种算法。
* `tspan` : 时间区间
* `kwargs` : 这里放一些`solver` 的关键字参数，比如容忍度。

接下来我们定义构造函数

```julia
function NeuralODE(model::Lux.AbstractExplicitLayer;
                   solver=Tsit5(),
                   sensealg=InterpolatingAdjoint(; autojacvec=ZygoteVJP()),
                   tspan=(0.0f0, 1.0f0),
                   kwargs...)
    return NeuralODE(model, solver, sensealg, tspan, kwargs)
end
```

这里的`InterpolatingAdjoint(; autojacvec=ZygoteVJP())` 就是[5.1.2.4](../5.-shen-jing-wei-fen-fang-cheng-shu-zhi-jie/5.1-chuan-guo-odes-jin-hang-fan-xiang-chuan-bo/5.1.2-xian-you-hua-zai-li-san/5.1.2.4-cha-zhi-ban-sui.md)节中的插值伴随，而参数`ZygoteVJP()` 表示用`Zygote` 这个自动微分库计算伴随矩阵的右端。

接下来定义前向传播函数

```julia
function (n::NeuralODE)(x, ps, st)
    function dudt(u, p, t)
        u_, st = n.model(u, p, st)
        return u_
    end
    prob = ODEProblem{false}(ODEFunction{false}(dudt), x, n.tspan, ps)
    return solve(prob, n.solver; sensealg=n.sensealg, n.kwargs...), st
end
```

这里的代码是不言自明的：我们定义动力学，打包成ODEProblem，然后传给求解器。

> `ODEfunction{false}` 表示它Out-Of-Place定义的，具体见[文档](https://diffeq.sciml.ai/stable/types/ode\_types/#SciMLBase.ODEFunction)

求解完毕之后，会返回一个\`ODESolution\`对象，我们要把它转化成数组

```julia
function diffeqsol_to_array(x::ODESolution{T, N, <:AbstractVector{<:CuArray}}) where {T, N}
    return dropdims(gpu(x); dims=3)  #如果使用GPU
end
diffeqsol_to_array(x::ODESolution) = dropdims(Array(x); dims=3) #如果使用CPU
```

### 创建和初始化模型

```julia
function create_model()
    # Construct the Neural ODE Model
    model = Chain(FlattenLayer(),
                  Dense(784, 20, tanh),
                  NeuralODE(Chain(Dense(20, 10, tanh), Dense(10, 10, tanh),
                                  Dense(10, 20, tanh));
                            save_everystep=false,
                            reltol=1.0f-3,
                            abstol=1.0f-3,
                            save_start=false),
                  diffeqsol_to_array,
                  Dense(20, 10))

    rng = Random.default_rng()
    Random.seed!(rng, 0)

    ps, st = Lux.setup(rng, model)
    ps = ComponentArray(ps) |> gpu
    st = st |> gpu

    return model, ps, st
end
```

我们先解释下`Chain` 这个结构体，它的输入和每个神经层一样，都是`(x,ps,st)` ，但只有`x` 会在内部顺序传递，而`ps` 和 `st`是关于整个`Chain` 的命名数组

```julia
julia> ps
ComponentVector{Float32, CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Tuple{Axis{(layer_1 = 1:0, layer_2 = ViewAxis(1:15700, Axis(weight = ViewAxis(1:15680, ShapedAxis((20, 784), NamedTuple())), bias = ViewAxis(15681:15700, ShapedAxis((20, 1), NamedTuple())))), layer_3 = ViewAxis(15701:16240, Axis(layer_1 = ViewAxis(1:210, Axis(weight = ViewAxis(1:200, ShapedAxis((10, 20), NamedTuple())), bias = ViewAxis(201:210, ShapedAxis((10, 1), NamedTuple())))), layer_2 = ViewAxis(211:320, Axis(weight = ViewAxis(1:100, ShapedAxis((10, 10), NamedTuple())), bias = ViewAxis(101:110, ShapedAxis((10, 1), NamedTuple())))), layer_3 = ViewAxis(321:540, Axis(weight = ViewAxis(1:200, ShapedAxis((20, 10), NamedTuple())), bias = ViewAxis(201:220, ShapedAxis((20, 1), NamedTuple())))))), layer_4 = 16241:16240, layer_5 = ViewAxis(16241:16450, Axis(weight = ViewAxis(1:200, ShapedAxis((10, 20), NamedTuple())), bias = ViewAxis(201:210, ShapedAxis((10, 1), NamedTuple())))))}}}(layer_1 = Float32[], layer_2 = (weight = Float32[-0.07626019 0.03154645 … -0.002694028 0.017019354; -0.0077336263 -0.06915471 … -0.035326436 -0.022705922; … ; 0.019855382 -0.0206198 … -0.019995632 0.02258391; 0.029717186 -0.06611487 … -0.006130313 0.004187409], bias = Float32[0.0; 0.0; … ; 0.0; 0.0;;]), layer_3 = (layer_1 = (weight = Float32[0.15751892 -0.07874616 … -0.31842813 0.03832691; -0.30397716 0.24566843 … -0.13367736 -0.00078198063; … ; 0.21275353 -0.291626 … -0.3417341 0.050843667; 0.13920508 0.06177098 … -0.0674695 -0.39660925], bias = Float32[0.0; 0.0; … ; 0.0; 0.0;;]), layer_2 = (weight = Float32[-0.5265294 0.15295507 … -0.065704055 -0.028035317; -0.33304396 0.065271355 … 0.02129917 0.38589293; … ; 0.22935219 0.4130424 … 0.12143973 -0.10436545; 0.14682935 0.4664488 … -0.28180373 -0.14330852], bias = Float32[0.0; 0.0; … ; 0.0; 0.0;;]), layer_3 = (weight = Float32[-0.35680208 -0.21872044 … -0.4038915 0.3286498; 0.39988494 0.30819586 … 0.103120364 -0.119971916; … ; -0.31085777 -0.30818656 … 0.41768086 0.08601755; -0.04838401 -0.13440865 … -0.15256278 -0.07446164], bias = Float32[0.0; 0.0; … ; 0.0; 0.0;;])), layer_4 = Float32[], layer_5 = (weight = Float32[0.122613475 -0.4459743 … -0.08683135 -0.40705428; 0.3358803 -0.35304752 … -0.3069419 0.07820547; … ; 0.26441804 -0.35173646 … -0.3781367 -0.14109525; -0.004071492 0.0052372124 … -0.3513142 -0.19022164], bias = Float32[0.0; 0.0; … ; 0.0; 0.0;;]))
```

```julia
julia> st
(layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple()), layer_4 = NamedTuple(), layer_5 = NamedTuple())
```

当然，在这里因为要和神经ODE结合用需要将命名数组转化成`ComponentVector`

你这个看到`st` 都是空的。大部分的神经网络不会记录任何状态信息，虽然它们都有统一的语法

```julia
y, st = model(x,ps,st)
```

但大部分时候`model` 内部不会对`st` 做出任何更改，只会原封不动的返回它。只有特定的一些层有非空的`st` ，例如[BatchNorm](https://github.com/avik-pal/Lux.jl/blob/74d2d39bac3c5d396f953f8eab6465207d52453b/src/layers/normalize.jl#L3-L69)

Chain的前向传播等价于

```julia
 for l in keys(model.layers)
       x = model.l(x, ps.l, st.l)
 end
```

Chain一个很好的功能我们可以很自然地添加任何无状态且无参数的函数，例如上面的`diffeqsol_to_array`。具体使用了[`WrappedFunction`](http://lux.csail.mit.edu/dev/api/layers/#Lux.WrappedFunction) ，它把我们的函数`f`打包以符合`y,st=f(x,ps,st)` 的格式。当然在这里`ps` 和`st` 都是空命名数组。

```julia
julia> model.layers
(layer_1 = FlattenLayer(), layer_2 = Dense(784 => 20, tanh_fast), layer_3 = NeuralODE(), layer_4 = WrappedFunction(diffeqsol_to_array), layer_5 = Dense(20 => 10))
```

再看

```julia
    rng = Random.default_rng()
    Random.seed!(rng, 0)

    ps, st = Lux.setup(rng, model)

```

这段代码就是初始化神经网络。因为神经网络和参数分离了，实例化神经网络并不会初始化参数。

### 训练

```julia
get_class(x) = argmax.(eachcol(x))

logitcrossentropy(ŷ, y) = mean(-sum(y .* logsoftmax(ŷ); dims=1))

function loss(x, y, model, ps, st)
    ŷ, st = model(x, ps, st)
    return logitcrossentropy(ŷ, y)
end

function accuracy(model, ps, st, dataloader)
    total_correct, total = 0, 0
    st = Lux.testmode(st)
    iterator = CUDA.functional() ? CuIterator(dataloader) : dataloader
    for (x, y) in iterator
        target_class = get_class(cpu(y))
        predicted_class = get_class(cpu(model(x, ps, st)[1]))
        total_correct += sum(target_class .== predicted_class)
        total += length(target_class)
    end
    return total_correct / total
end

function train()
    model, ps, st = create_model()

    # Training
    train_dataloader, test_dataloader = loadmnist(128, 0.9)

    opt = Optimisers.ADAM(0.001f0)
    st_opt = Optimisers.setup(opt, ps)

    nepochs = 9
    for epoch in 1:nepochs
        stime = time()
        iterator = CUDA.functional() ? CuIterator(train_dataloader) : train_dataloader
        for (x, y) in iterator
            gs = gradient(p -> loss(x, y, model, p, st), ps)[1]
            st_opt, ps = Optimisers.update(st_opt, ps, gs)
        end
        ttime = time() - stime

        println("[$epoch/$nepochs] \t Time $(round(ttime; digits=2))s \t Training Accuracy: " *
                "$(round(accuracy(model, ps, st, train_dataloader) * 100; digits=2))% \t " *
                "Test Accuracy: $(round(accuracy(model, ps, st, test_dataloader) * 100; digits=2))%")
    end
end

train()
```

